{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서플로,케라스 cpu, gpu 선택을 위해 지원되는 디바이스 넘보 보기\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output # clear_output() 으로 아웃풋 제거 가능\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings # 경고가 너무 많이뜨는경우 주피터가 죽음.... 아래 주석 해제하여 워닝끄면 됨\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "import sklearn as sk\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png','retina'}\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_color_codes()\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#한글폰트 적용\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "#음수처리\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수업 git\n",
    "- https://github.com/solaris33/dl_cv_tensorflow_10weeks\n",
    "\n",
    "## 수업 자료\n",
    "- http://solarisailab.com/fastcampus_dl_cv_10weeks_(6th).html\n",
    "\n",
    "## 강사님 딥러닝 블로그\n",
    "- http://solarisailab.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ※ 1주차 수업이후 해볼것\n",
    "\n",
    ">1. *[완료]수업에서 사용하는 git 저장소 훑어보기\n",
    ">2. *[완료]NN종류 및 사용용도 등 정리하기 (수업 이외의 현재까지 진보된 모든 NN들 찾아보기!)\n",
    ">3. *[완료!]1주차 수업자료 모두 훑어보기*\n",
    ">4. *[완료!]텐서플로로 여러가지 공부, 코드 눈으로 익히기 및 학습하기\n",
    "    - 활성화함수 종류 찾아보고 정리 sigmoid, rely 등등 (강의자료 참고해서 찾아보자)\n",
    "    - 분류기 종류 찾아보고 정리 softmax 등등\n",
    "    - 최적화함수 종류 찾아보고 정리 adam, gradientdecent, 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ◆ Deep Learning의 정의\n",
    "1. 딥러닝은 깊은 학습이다\n",
    "2. 딥러닝은 Feature Learning이다(특징학습)\n",
    "3. deep learning(CNN,LSTM) < machine learning(로지스틱,svm) < artificial intelligence\n",
    "\n",
    "### ※ 딥러닝의 역사\n",
    "- 1세대 : 1943~1986 McCulloch, Warren S., Walter pitts 가 인공신경망의 아이디어를 제안\n",
    "    - 1958년 Frank Rosenblatt이 퍼셉트론 모형을 제안\n",
    "    - 1969년 퍼셉트론 모델의 한계 지적됨(선형분리 불가능) --> 인기가 사그라듬\n",
    "- 2세대 : 1986~2006 Multi-Layer Perceptron(MLP)와 Backpropagation 알고리즘 제안, XOR문제 해결됨(선형분리 가능)\n",
    "    - 1998년 Yann Lecun의 CNN 구조인 LeNet은 수표인식에 실용적으로 사용됨\n",
    "    - But. Layer를 깊게 쌓을수록 Gradient가 사라지는 Vanishing Gradient Problem발생\n",
    "    - 이로인해 Deep Learning은 불가능 --> SVM과 같은 기법이 탐구됨\n",
    "- 3세대 : 2006~2012 Geoffrey Hinton과 같은 딥러닝 선구자들의 지속적인 연구\n",
    "    - 2006년 Hinton은 pre-training을 이용해 학습한 Deep Autoencoders의 논문을 발표\n",
    "    - 이후 RBM, Pre-training, Dropout, ReLU등의 오버피팅 방지 알고리즘들 등장\n",
    "    - 학습데이터를 손쉽게 얻을 수 있는 인터넷과 대용량처리를 할수있는 GPU의 발전 --> 딥러닝 기법의 부흥\n",
    "    - 2012년 AlexNet이 이미지 분류 대회에서 압도적 성능으로 우승을 차지함 --> 이로써 글로벌 대기업의 자본들이 딥러닝에 집중됨\n",
    "- 4세대 : 2012~현재\n",
    "    - DeepMind는 Deep ANNs와 강화학습을 결합한 Deep-Q-Networks(DQN)을 제안 --> 강화학습에 대한 관심을 불러일으킴\n",
    "    - 최근에는 GAN에 대한 연구가 활발히 진행중\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ◆ 머신러닝의 세가지 분류\n",
    "1. 지도학습(supervised) - labeled data, direct feedback, predict outcome/future\n",
    "2. 비지도학습(unsupervised) - no labels, no feedback, fine hidden structure\n",
    "3. 강화학습(reinforcement) - decision process, reward system, learn series of action\n",
    "\n",
    "## ◆ 머신러닝의 기본 프로세스\n",
    "1. 학습하고자하는 가성(hypothesis) θ을 수학적 표현식으로 나타낸다.\n",
    "2. 성능을 측정할 수 있는 비용함수(cost function) J(θ)을 정의한다.\n",
    "3. cost function을 최소화 할 수 있는 학습알고리즘을 설계한다.\n",
    "\n",
    "### ▶cost function을 최소화 하는 알고리즘\n",
    "- Gradient Descent\n",
    "    - Stochastic Gradient Descent(Mini-batch GD) - 각 회차별로 샘플설정\n",
    "         - batch GD = 각 회차에 모든샘플을 사용\n",
    "         - mini-batch GD = 각 회차마다 n개의 샘플을 사용\n",
    "         - stochastic GD = 각 회차마다 1개의 샘플을 사용\n",
    "- Cross-entropy\n",
    "    - Classification 문제에서 softmax regression모델을 사용할결우 cost function으로 자주 사용됨.\n",
    "    - 모델의 예측값(prediction)이 실제 참(truth)값을 설명하는 데 얼마나 비효율적인지 나타냄.\n",
    "    - cross-entropy가 낮을수록 좋은모델이다. --> cost 최소화하는 방향의 학습!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ◆ `딥러닝 출발점인 ANN (Artificial NN)`\n",
    "- MultiLayer Perceptron(MLP)\n",
    "    - 각각의 질문들은 더 구체적이고 작은 질문들로 분헤(Decompose) 될 수 있다. --> 이 과정은 Weight조정을 통해서 이루어진다.\n",
    "    \n",
    "    \n",
    "## ◆ `다양한 NN구조들`\n",
    ">- ### Autoencoder\n",
    "    - input unit 개수와 output unit 개수가 동일한 구조의 NN\n",
    "    - Hidden Layer는 input정보를 압축적으로 저장 --> 특징(Feature)학습\n",
    "    - 응용분야 : Feature Extraction\n",
    "    - 특징 : No Label 데이터에 대한 비지도학습이 가능\n",
    " \n",
    ">- ### Convolutional Neural Networks(CNNs)\n",
    "    - 응용분야 : 컴퓨터 비전\n",
    "    - 장점 : 이미지와 같이 차원이 큰 데이터도 다룰수 있게됨\n",
    "    - 단점 : Pooling 과정에서 정보가 손실됨\n",
    "    - R-CNN, Fast R-CNN, Faster R-CNN \n",
    "    \n",
    ">- ### Recurrent Neural Networks(RNNs)\n",
    "    - 응용분야 : 자연어처리\n",
    "    - 장점 : 시간축이 추가됨으로써 시간적 연속성을 지닌 데이터를 잘 처리할 수 있게됨, \n",
    "        - 이전시간의 상태에 대한 정보를 일종의 메모리 형태로 가지고 있을 수 있음\n",
    "    - 단점 : 연산량의 증가, Vanishing Gradient Problem(시간이 지나면서 이전 시간의 영향력이 작아지다가 사라지는 문제)\n",
    "    \n",
    ">- ### Long-Short Term Memory(LSTM) Networks\n",
    "    - RNN의 Vanishing Gradient Problem을 해결\n",
    "    - 응용분야 : 자연어처리(시계열)\n",
    "    - 장점 : RNNs에 비해 좀더 긴 시간의 dependency를 저장할 수 있음(장기기억 가능)\n",
    "    - 단점 : 연산량이 증가함\n",
    "    \n",
    ">- ### Generative Adversarial Networks(GAN)\n",
    "    - Generator(생성자) vs Discriminator(구분자) --> 가짜를 구분하지 못할정도로 진짜처럼 생성하는걸 목표로 하는 강화학습 기법\n",
    "    - 응용분야 : 이미지 생성(Image Generation)\n",
    "    - 장점 : 새로운 이미지 생성가능\n",
    "    - 단점 : 학습의 어려움 --> DC GAN이 모델 구조를 개선하여 고해상도 생성 가능해짐\n",
    "    - GAN의 근본적인 문제인 loss의 불안정성을 해결한 Wassertein GAN, D를 AE로 바꾼 EBGAN 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ◆ `시기별 NN구조들`\n",
    "\n",
    "- 각 NN의 역사 및 자세한 설명, 결과비교 블로그(https://laonple.blog.me/220782324594)\n",
    "\n",
    "\n",
    ">- ### LeNet\n",
    "    - 1990년대에 Yann LeCun이 CNN이라는 개념을 성공적으로 도입\n",
    "\n",
    ">- ### AlexNet ★ (2012년 거의 최초의 이미지 분류기!)\n",
    "    - 2012년 ILSVRC대회에서 오차율을 최소화 하여 우승 --> 이미지 분류에 CNN이 대세로 쓰이게 된 계기\n",
    "    - Lenet은 conv레이어 뒤에 pooling이 오는 구조, AlexNet은 conv레이어 뒤에 바로 conv가 위치함\n",
    "\n",
    ">- ### ZF Net\n",
    "    - 2013년 ILSVRC대회에서 우승\n",
    "    - AlexNet의 하이퍼 파라미터를 수정하여 성능개선, 중간의 conv레이어의 크기를 늘림\n",
    "\n",
    ">- ### GoogleLeNet\n",
    "    - 2014년 ILSVRC대회에서 우승\n",
    "    - Inception Module개념 도입 --> 파라미터의 수를 대폭 줄임 (AlexNet 60M --> GoogleLeNet 4M)\n",
    "    \n",
    ">- ### VGGNet ★ (많이사용중)\n",
    "    - 2014년 ILSVRC대회에서 2등\n",
    "    - 단순한 구조(3x3 conv, 2x2 max pooling) + 16depth ==> 2등???!!!\n",
    "    - 다중 전달학습과제에서는 GoogleLeNet보다 우수\n",
    "    - CNN연구그룹에서는 VGGNet의 구조를 선호하는 경향\n",
    "    - 메모리수, 파라미터수가 크다는게 단점\n",
    "    \n",
    ">- ### ResNet ★★ (CNN대세)\n",
    "    - 2015년 ILSVRC대회에서 우승\n",
    "    - residual framework 개념 도입 \n",
    "    - --> Deep한 레이어의 경우 오류 역전파가 사라지는 것 방지 --> 더 개선된 성능!!\n",
    "    - --> CNN에서는 이제 사람수준으로 오류를 판별하고 더 나은 모습을 보여줄 정도.\n",
    "    - ----> **Faster R-CNN**과의 결합으로 신의경지에 오름 ㅋㅋㅋㅋ\n",
    "    - ------> classification, detection, localization 모든 경지에서 우승!\n",
    "    \n",
    "    \n",
    "## ◆ `시중에 나온 성능좋은 NN구조들`\n",
    "\n",
    ">- ### Inception - (GoogleLenet 파생상품[?])\n",
    "    - 2016년에 ResNet의 일부 강점을 활용만한(??) v4가 나왔음\n",
    "    - 이미지인식에 뛰어난 신경망모델 현재 v4까지 나왔고, ResNet과 박빙 \n",
    "    - Inception이라는 개념을 사용함\n",
    " \n",
    ">- ### \n",
    " \n",
    ">- ### DQN - 강화학습 (딥마인드)\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ◆ 효율적인 학습테크닉\n",
    "\n",
    "## ▶ 데이터 전처리단 -----------------------------------\n",
    "### - image Augmentation\n",
    ">- 오그멘테이션의 종류 및 활용성 http://nittaku.tistory.com/272?category=742607\n",
    "- keras 코드 https://datascienceschool.net/view-notebook/0378a43f48a742988897a6629add11fe/\n",
    "\n",
    "## ▶ 학습 및 레이어단 -------------------------------------\n",
    "### - Dropout  (DropConnect)\n",
    ">- 오버피팅을 줄이기 위한 여러 기법 (출처 https://laonple.blog.me/220818841217)\n",
    "- 2012년에 발표된 regularization방법 \n",
    "- overfitting을 방지하기위해선 거의 필수적인것으로 자리잡아왔었음.\n",
    "- BN이 발표되고 난 이후에는 궂이 DropOut을 쓸 필요가 없다는 주장이 있음.(쓸지는 내 결정에 따름)\n",
    "- **DropConnect**방식이 이후에 등장 --> 이것이 DropOut을 쓸때보다 쪼금 더 성능이 좋긴하다.\n",
    "\n",
    "\n",
    "### - Batch Normalization ★(일단은 그냥 무조건 써야된다... 현재는 이거안쓰면 힘들어..)\n",
    ">- 자세한 설명은 링크참조!(출처 https://laonple.blog.me/220808903260)\n",
    "- 2015년도에 획기적인 방법 두개중 하나가 BN이다. (다른하나는 Residual Net^^... 응 공부하자)\n",
    "- 보통 mini-batch SGD 에서는 32~ 256사이의 값을 사용함.(그래픽카드를 효율적으로 쓰기위해, 지금은 512도 될듯??)\n",
    "- Covariance shift의 문제를 막기위한 두개의 방법중 하나다.(다른 하나는 whitening(백색잡음))\n",
    "\n",
    "### - Pooling (Stochastic-pooling)\n",
    ">- 자세한설명 링크참조(출처 https://laonple.blog.me/220830178487)\n",
    "- 보통 max-pooling을 쓰긴 한다. 그러나아... overfitting문제가 있다.\n",
    "- 그래서 나온게 스토캐스틱 풀링!\n",
    "- --> 확률에 따라 적절한 activation을 선택!!\n",
    "- ---->가장 강한 자극만을 선택하는것이 아닌 최댓값이 아닌 것이라도 정보가 더 중요하면 그 정보를 채택!\n",
    "- --> 사용하면 iteration이 길어질수록 점점 에러율이 줄어들며 학습효과가 있음을 보인다 => 좋은 해결법!\n",
    "- 아 keras에 stochastic 풀링이 없넹..... \n",
    "- 트레이닝시에는 stochastic pooling, 테스트시엔 Probability Weighting을 쓰면 에러 최소화 굳굳\n",
    "\n",
    "### - MaxOut (일종의 활성화함수임)\n",
    "> - 그래 링크를 보자(출처 https://laonple.blog.me/220836305907)\n",
    "- **DropOut**의 효과를 극대화하기위해 독특한 활성함수를 고안 --> 성능에 큰 영향을 준다! (오호?)\n",
    "- 활성화 함수지만 좀 독특한 구조이다. ReLU보다 최적화에 유리하고, tanh보다 에려율이 현저히 낮다. 만능인가?\n",
    "- Maxout + DropOut하면 아주 성능향상이... 꼭 써보고싶다...진짜 써보고싶다\n",
    "\n",
    "### - 활성화함수(Activation Functions)\n",
    ">- Sigmoid\n",
    "- tanh\n",
    "- ReLU --> 최근에 제안된 활성화함수 = sigmoid의 문제점을 해결하기 위해 제시됨 --> sigmoid보다 6배 빠른 학습속도\n",
    "- Leaky ReLU\n",
    "- Maxout\n",
    "- ELU\n",
    "- Softmax : 모두 더하면 1이되는 normalized 된 함수임, output은 각각의 Lable에 대해 예측한 확률값이 됨\n",
    "    - 주로 Classification 문제의 마지막 Layer에 사용\n",
    "\n",
    "### - 최적화함수(Optimizer)\n",
    "- 그림설명이 있으니 참고(출처 http://seamless.tistory.com/38)\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/993D383359D86C280D)\n",
    "![](http://i.imgur.com/pD0hWu5.gif?1)\n",
    "\n",
    "> - **Gradient Descent** \n",
    "    - 가장 기본적인 방식, 기울기의 반대방향으로 그 크기에 비례하게 이동하여 최저값을 찾는다.\n",
    "    - 정확하긴 하지만 너무너무너무너무너무너무 느리다는 단점....\n",
    "- **Momentum** (방향성)\n",
    "    - GD에 일종의 관성원리를 추가(r값으로 0.9정도 준다) => GD보다 좋은성능\n",
    "    - but 이전벡터의 정보 저장 => 필요용량 2배증가\n",
    "- **Adagrad** (스텝사이즈 고려)\n",
    "    - 각 변수의 stepsize를 다르게 하여 이동 => 빠르게 loss를 줄일때 효과적 => lr을 고려안해도 된다\n",
    "    - but lr값이 너무 작아져 iteration이 커지게 되면 거의 움직이지 않는 단점\n",
    "- **Adadelta ★** (스텝사이즈 고려)\n",
    "    - adagrad의 단점 보완. G값을 구할때 합을 구하는대신 지수평균 이용 --> 너무 작아지는 단점 극복\n",
    "    - but 딥러닝에 사용시 성능이 더 안좋을때도 종종 있음\n",
    "- **RMSProp ★** (스텝사이즈 고려)\n",
    "    - adagrad의 단점 보완하기위해 adadelta와 함께 등장\n",
    "    - 보통 성능이 Adagrad, adadelta보다 나음\n",
    "    - momentum을 적용할 수 있음\n",
    "    - adam과 함께 많이 사용되는 옵티마이저임.\n",
    "- **Adam ★** (방향성 + 스텝사이즈)\n",
    "    - RMS와 momentum의 장점을 합침\n",
    "    - 모든 장점이 결합되어 최고의 성능을 보여줌.\n",
    "    - 가장 많이 쓰이는 최적화 알고리즘이다.\n",
    "- **Nadam ★** (방향성 + 스텝사이즈)\n",
    "    - adam에 방향성을 먼저 고려하도록 momentum대힌 NAG를 적용\n",
    "    - 성능 나쁘지 않은편\n",
    "    \n",
    "  \n",
    "## ▶학습전후 조정단 ---------------------------------------\n",
    "\n",
    "### - 하이퍼파라미터 조정\n",
    "> 1. Manual Search\n",
    "    - 직관이나 경험에 기반한 파라미터 추정, 결과를 관찰\n",
    "    - 특정 탐색이론을 따르며, 어떤 이론을 따르냐에 따라 시간과 질이 달라진다.\n",
    "    - validation set이 필요함\n",
    "> 2. Grid Search (parameter sweep)\n",
    "    - 개념은 메뉴얼써치와 비슷, 선험적 지식을 활용하고, 파라미터의 범위를 정한다.\n",
    "    - 범위안에서 일정한 간격으로 점을 정한다 -> 점들에 대해 1개씩 차례로 실험, 최적의 값을 찾는다 -> \n",
    "    - best를 찾으면 그 점을 기준으로 다시 세분화하여 최적값 찾는다.\n",
    "    - validation set이 필요함\n",
    "> 3. Random Search\n",
    "    - 무작위로 최적값을 찾는작업\n",
    "    - 시간적 제한이 있는경우에는 위의 두 방법보다는 random search가 좋은결과를 내는 경향이 있다.\n",
    "> 4. Bayesian optimization ★\n",
    "    - 실험결과를 바탕으로 통계적 모델을 만들고, 이를 바탕으로 다음 탐색을 해야할 방향을 정하는것.\n",
    "    - Random, Grid search보다 더 짧은 시간에 최적값을 찾아내는 경향이 있음.\n",
    "    \n",
    "- Fine-tuning(Transfer Learning) : 이미 학습된 NN을 새로운 Task에 맞게 다시 미세조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ◆ 컴퓨터비전 문제/예제들\n",
    "> ### 기초\n",
    "    - Image Classification (MNIST숫자분류, CIFAR-10이미지분류)\n",
    "    - Inception v3 Retraining\n",
    "    - 한글 Char-RNN(한글 희곡생성)\n",
    "    - Visual Q&A\n",
    "    - Image Captioning\n",
    "    \n",
    "> ### 중급\n",
    "    - Deconvolution, DeepDream, Neural Style Transfer\n",
    "    - Generative Model - Variational AutoEncoder(VAE) & Generative Adversarial Networks(GAN)\n",
    "\n",
    "> ### 고급\n",
    "    - AutoPilot (자가운전)\n",
    "    - Semantic Image Segmentation (이미지 형상 분류)\n",
    "    - Brain Tumor Segmentation (뇌CT 종양 진단)\n",
    "    - Object Detection - YOLO --> R-CNN, Fast R-CNN, Faster R-CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 텐서플로우를 사용해야하는 이유?\n",
    "- 파이썬 API\n",
    "- single API로 cpu/gpu, single/multi, server/laptop 다 커버 가능\n",
    "- 유연성 : 라즈베리파이, 안드로이드, 윈도우, ios, 리눅스 등등\n",
    "- 시각화 : 텐서보드 지린다\n",
    "- 체크포인트 : 실험관리 가능!\n",
    "- 자동 맞춤\n",
    "- 커뮤니티 개크다\n",
    "- 대단한 실험들이 이미 많이 텐서플로로 만들어져있다.\n",
    "\n",
    "## 텐서플로는?\n",
    "- TensorFlow는 데이터를 Graph형태(Node, Edge)로 나타낸다.\n",
    "- Node : operators, variables, and constants\n",
    "- Edge : tensors\n",
    "- tensor는 모든 n차원 행렬을 말한다.\n",
    "- 텐서플로 실행과정\n",
    "    1. 그래프 생성\n",
    "    2. Session을 이용한 그래프 상의 연산 수행\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로 및 여러가지 딥러닝모델 기초 작성 순서\n",
    "1. **가설정의(hypothesis)**\n",
    "\n",
    ">- tf.Variable\n",
    "- tf.placeholder\n",
    "- logits = tf.matmul\n",
    "    \n",
    "2. **손실함수정의(loss(cost) function)**\n",
    "\n",
    ">- tf.nn.cross_entropy....\n",
    "- tf.nn.reduce_mean....\n",
    "    \n",
    "3. **최적화정의(optimizer)**\n",
    "\n",
    ">- tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "- tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "    \n",
    "4. **학습(learning)**\n",
    "\n",
    ">- sess = tf.Session()\n",
    "- sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "5. **모델정확도 측정(prediction and validation)**\n",
    "\n",
    ">- prediction = tf.equal(tf.argmax....))\n",
    "- accuracy = tf.reduce_mean(tf.cast(....))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# ※ 2주차 수업이후 해볼것\n",
    "\n",
    ">1. 수업에서 사용하는 git 저장소 훑어보기\n",
    ">2. 2주차 수업자료 모두 훑어보기\n",
    ">3. CNN 이미지 학습시 알아야할 여러 기법들의 종류 및 사용용도 정리 (수업 이외의 현재까지 진보된 모든 방법들 찾아보기!)\n",
    "    - ReLU가 가장 좋은가? 찾아보기\n",
    "    - Fine-Tuning?\n",
    "    - 앙상블러닝모델? --> 활용도는 대회정도? 에러율을 1%까지 짜내야 되는 수준이 아니면 궂이 사용할 필요까지는 없다\n",
    "    - 데이터 오그멘테이션\n",
    "    - PCA및 화이트닝, 노멀라이징, 평균이미지 빼기 or 채널별 평균 빼기 등...\n",
    "    - LRN은 성능에 영향을 주지 않음\n",
    "    - GoogleLenet 핵심 idea : \n",
    ">4. 코드 예습 및 복습하기~!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1x1 convolution을 쓸때의 장점?\n",
    "    - 차원축소\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구글Lenet 코드\n",
    "1. 디멘션 받기\n",
    "    - tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "2. 풀링, 드롭아웃\n",
    "    - tf.nn.max_pool\n",
    "    - tf.nn.dropout\n",
    "3. 세이버(저장기능)    \n",
    "    - tf.nn.train.Saver() --> ckpt(체크포인트)파일로 저장됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
