{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png','retina'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다중출력여부\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우 튜토리얼 - 기본기 연습 및 간단한 모델\n",
    "- 튜토리얼을 제공하는 깃허브 https://github.com/golbin/TensorFlow-Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ◆ NeuralNetwork Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 자연어 분석에 매우 중요하개 사용되는 Word2Vec모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#한글폰트 적용\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "#음수처리\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 단어 벡터를 분석해볼 임의의 문장\n",
    "sentences = [\"나 고양이 좋다\",\n",
    "             \"나 강아지 좋다\",\n",
    "             \"나 동물 좋다\",\n",
    "             \"강아지 고양이 동물\",\n",
    "             \"여자친구 고양이 강아지 좋다\",\n",
    "             \"고양이 생선 우유 좋다\",\n",
    "             \"강아지 생선 싫다 우유 좋다\",\n",
    "             \"강아지 고양이 눈 좋다\",\n",
    "             \"나 여자친구 좋다\",\n",
    "             \"여자친구 나 싫다\",\n",
    "             \"여자친구 나 영화 책 음악 좋다\",\n",
    "             \"나 게임 만화 애니 좋다\",\n",
    "             \"고양이 강아지 싫다\",\n",
    "             \"강아지 고양이 좋다\"]\n",
    "\n",
    "# 문장을 전부 합친 후 공백으로 단어들을 나누고 고유한 단어들로 리스트를 만든다.\n",
    "word_sequence = \" \".join(sentences).split()\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "# 문자열로 분석하는 것 보다, 숫자로 분석하는 것이 훨씬 용이함\n",
    "# 리스트에서 문자들의 인덱스를 뽑아 사용하기 위해서\n",
    "# 이를 표현하기 위한 연관 배열과, 단어 리스트에서 단어를 참조할 수 있는 인덱스 배열을 만든다\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "# 윈도우 사이즈를 1로하는 skip-gram모델을 만든다\n",
    "# 예) 나 게임 만화 애니 좋다\n",
    "#   -> ([나, 만화], 게임), ([게임, 애니], 만화), ([만화, 좋다], 애니)\n",
    "#   -> (게임, 나), (게임, 만화), (만화, 게임), (만화, 애니), (애니, 만화), (애니, 좋다)\n",
    "skip_grams=[]\n",
    "\n",
    "for i in range(1, len(word_sequence)-1):\n",
    "    # (context, target) : ([target index -1, target_index +1], target)\n",
    "    # 스킵그램을 만든 후, 저장은 단어의 고류 번호(index)로 저장\n",
    "    target = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i-1]],word_dict[word_sequence[i+1]]]\n",
    "    \n",
    "    # (target, context[0]),(target, context[1])...\n",
    "    for w in context:\n",
    "        skip_grams.append([target, w])\n",
    "\n",
    "# skip-gram 데이터에서 무작위로 데이터를 뽑아 입력값과 출력값의 배치 데이터를 생성하는 함수\n",
    "def random_batch(data,size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)),size, replace=False)\n",
    "    \n",
    "    for i in random_index:\n",
    "        random_inputs.append(data[i][0]) # target\n",
    "        random_labels.append([data[i][1]]) #context word\n",
    "        \n",
    "    return random_inputs, random_labels\n",
    "\n",
    "#########=======================================\n",
    "# 옵션 설정\n",
    "######\n",
    "# 학습을 반복할 횟수\n",
    "training_epoch = 1000\n",
    "# 학습률\n",
    "learning_rate = 0.1\n",
    "# 한번에 학습할 데이터의 크기\n",
    "batch_size = 20\n",
    "# 단어 벡터를 구성할 임베딩 차원의 크기\n",
    "# 이 예제는 x,y그래프로 표현하기 쉽도록 2개값만 출력\n",
    "embedding_size = 2\n",
    "# word2ved 모델 학습을 위한 nce_loss 함수에서 사용하기 위한 샘플링 크기\n",
    "# batch_size 보다 작아야 한다.\n",
    "num_sampled = 15\n",
    "# 총 단어갯수\n",
    "voc_size = len(word_list)\n",
    "\n",
    "#########=======================================\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# tf.nn.nce_loss를 사용하려면 출력값을 이렇게 [batch_size, 1] 구성해야한다.\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# word2vec 모델의 결과값인 임베딩 벡터를 저장할 변수다.\n",
    "# 총 단어 갯수와 임베딩 갯수를 크기로 하는 두개의 차원을 갖는다.\n",
    "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size],-1.0, 1.0))\n",
    "# 임베팅 벡터의 차원에서 학습할 입력값에 대한 행들을 뽑아온다\n",
    "# 예) embeddings     inputs    selected\n",
    "#    [[1, 2, 3]  -> [2, 3] -> [[2, 3, 4]\n",
    "#     [2, 3, 4]                [3, 4, 5]]\n",
    "#     [3, 4, 5]\n",
    "#     [4, 5, 6]]\n",
    "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# nce_loss 함수에서 사용할 변수들을 정의한다.\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "# nce_loss 함수를 직접 구현하려면 매우 복잡하지만,\n",
    "# 텐서플로우에서 제공하는 함수가 있으니 tf.nn.nce_loss함수를 사용하면됨\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "#########==========================================\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init= tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, training_epoch + 1):\n",
    "        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
    "        \n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                              feed_dict={inputs:batch_inputs,\n",
    "                                         labels:batch_labels})\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss at step \",step, \": \",loss_val)\n",
    "    \n",
    "    # matplot 으로 출력하여 시각적으로 확인해보기 위해\n",
    "    # 임베딩 벡터의 결과 값을 계한하여 저장한다.\n",
    "    # with 구문 안에서는 sess.run 대신 간단히 eval() 함수를 사용할 수 있다.\n",
    "    trained_embeddings = embeddings.eval()\n",
    "    \n",
    "#########=========================================\n",
    "# 임베딩된 Word2Vec 결과 확인\n",
    "# 결과는 해당 단어들이 얼마나 다른 단어와 인접해 있는지를 보여줍니다.\n",
    "######    \n",
    "for i, label in enumerate(word_list):\n",
    "    x,y = trained_embeddings[i]\n",
    "    plt.scatter(x,y)\n",
    "    plt.annotate(label, xy=(x,y), xytext=(5,2),\n",
    "                textcoords='offset points', ha='right', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN\n",
    "- 여러개의 신경망을 구성하는 방법 익혀보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.10388\n",
      "20 0.866431\n",
      "30 0.674503\n",
      "40 0.533089\n",
      "50 0.42709\n",
      "60 0.348081\n",
      "70 0.287232\n",
      "80 0.238058\n",
      "90 0.197017\n",
      "100 0.163432\n",
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "# 신경망의 레이어를 여러개로 구성하여 말로만 듣던 딥러닝을 구성해 봅시다!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########================================\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2,10]으로 정한다.\n",
    "W1 = tf.Variable(tf.random_uniform([2,10], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류갯수] -> [10,3] 으로 정한다.\n",
    "W2 = tf.Variable(tf.random_uniform([10,3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정한다.\n",
    "# b1은 히든 레이어의 뉴런 갯수로, b2는 최종 결과값 즉, 분류갯수인 3으로 설정\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용\n",
    "L1 = tf.add(tf.matmul(X, W1),b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산한다\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어낸다\n",
    "model = tf.add(tf.matmul(L1, W2),b2)\n",
    "\n",
    "# 덴서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용함수를 다음처럼 간단하게 적용할 수 있음\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########================================\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if (step + 1)% 10 == 0:\n",
    "        print(step+1, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "        \n",
    "        \n",
    "\n",
    "#########================================\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model,1)\n",
    "target = tf.arg_max(Y,1)\n",
    "prediction = tf.argmax(model,1)\n",
    "target = tf.argmax(Y,1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy*100, feed_dict={X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "- 신경망을 구성하여 간단한 분류 모델을 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.04917\n",
      "20 1.04776\n",
      "30 1.0465\n",
      "40 1.04516\n",
      "50 1.04391\n",
      "60 1.04267\n",
      "70 1.04139\n",
      "80 1.0402\n",
      "90 1.03936\n",
      "100 1.03854\n",
      "110 1.03775\n",
      "120 1.03698\n",
      "130 1.03619\n",
      "140 1.03545\n",
      "150 1.03467\n",
      "160 1.034\n",
      "170 1.03321\n",
      "180 1.0325\n",
      "190 1.0318\n",
      "200 1.03108\n",
      "210 1.0304\n",
      "220 1.02977\n",
      "230 1.02942\n",
      "240 1.02909\n",
      "250 1.02873\n",
      "260 1.02849\n",
      "270 1.02806\n",
      "280 1.02775\n",
      "290 1.0274\n",
      "300 1.02708\n",
      "310 1.02674\n",
      "320 1.02645\n",
      "330 1.02609\n",
      "340 1.02575\n",
      "350 1.02546\n",
      "360 1.02511\n",
      "370 1.02477\n",
      "380 1.02448\n",
      "390 1.02414\n",
      "400 1.02391\n",
      "410 1.02351\n",
      "420 1.02318\n",
      "430 1.0229\n",
      "440 1.02257\n",
      "450 1.02233\n",
      "460 1.02196\n",
      "470 1.02163\n",
      "480 1.02136\n",
      "490 1.02103\n",
      "500 1.02072\n",
      "510 1.02045\n",
      "520 1.02012\n",
      "530 1.01981\n",
      "540 1.01955\n",
      "550 1.01922\n",
      "560 1.01891\n",
      "570 1.01868\n",
      "580 1.01834\n",
      "590 1.01803\n",
      "600 1.01772\n",
      "610 1.01752\n",
      "620 1.01716\n",
      "630 1.01687\n",
      "640 1.01661\n",
      "650 1.01631\n",
      "660 1.01601\n",
      "670 1.01577\n",
      "680 1.01552\n",
      "690 1.01518\n",
      "700 1.01489\n",
      "710 1.01464\n",
      "720 1.01436\n",
      "730 1.01408\n",
      "740 1.01377\n",
      "750 1.01349\n",
      "760 1.01329\n",
      "770 1.01296\n",
      "780 1.01268\n",
      "790 1.01241\n",
      "800 1.01219\n",
      "810 1.01193\n",
      "820 1.01162\n",
      "830 1.01134\n",
      "840 1.01107\n",
      "850 1.0108\n",
      "860 1.01053\n",
      "870 1.01037\n",
      "880 1.01004\n",
      "890 1.00976\n",
      "900 1.00951\n",
      "910 1.00924\n",
      "920 1.00898\n",
      "930 1.00876\n",
      "940 1.00856\n",
      "950 1.00826\n",
      "960 1.00798\n",
      "970 1.00773\n",
      "980 1.00747\n",
      "990 1.00724\n",
      "1000 1.00696\n",
      "1010 1.00675\n",
      "1020 1.00648\n",
      "1030 1.00625\n",
      "1040 1.00602\n",
      "1050 1.00575\n",
      "1060 1.00551\n",
      "1070 1.00527\n",
      "1080 1.00502\n",
      "1090 1.00478\n",
      "1100 1.0046\n",
      "1110 1.0043\n",
      "1120 1.00407\n",
      "1130 1.0039\n",
      "1140 1.00365\n",
      "1150 1.00346\n",
      "1160 1.00322\n",
      "1170 1.00292\n",
      "1180 1.00269\n",
      "1190 1.00249\n",
      "1200 1.00225\n",
      "1210 1.00202\n",
      "1220 1.00178\n",
      "1230 1.00155\n",
      "1240 1.00132\n",
      "1250 1.00109\n",
      "1260 1.00087\n",
      "1270 1.00064\n",
      "1280 1.00042\n",
      "1290 1.00021\n",
      "1300 0.999989\n",
      "1310 0.999774\n",
      "1320 0.999562\n",
      "1330 0.999352\n",
      "1340 0.999144\n",
      "1350 0.998939\n",
      "1360 0.998687\n",
      "1370 0.998471\n",
      "1380 0.998334\n",
      "1390 0.998121\n",
      "1400 0.99788\n",
      "1410 0.997676\n",
      "1420 0.997392\n",
      "1430 0.997188\n",
      "1440 0.997013\n",
      "1450 0.996775\n",
      "1460 0.996565\n",
      "1470 0.996377\n",
      "1480 0.996168\n",
      "1490 0.99596\n",
      "1500 0.995774\n",
      "1510 0.995604\n",
      "1520 0.995362\n",
      "1530 0.99521\n",
      "1540 0.994948\n",
      "1550 0.994798\n",
      "1560 0.994548\n",
      "1570 0.994359\n",
      "1580 0.994163\n",
      "1590 0.993971\n",
      "1600 0.993815\n",
      "1610 0.99366\n",
      "1620 0.993438\n",
      "1630 0.993184\n",
      "1640 0.992987\n",
      "1650 0.992811\n",
      "1660 0.992611\n",
      "1670 0.992461\n",
      "1680 0.992319\n",
      "1690 0.99211\n",
      "1700 0.991879\n",
      "1710 0.991664\n",
      "1720 0.991484\n",
      "1730 0.991328\n",
      "1740 0.991196\n",
      "1750 0.990999\n",
      "1760 0.990741\n",
      "1770 0.99057\n",
      "1780 0.990383\n",
      "1790 0.990205\n",
      "1800 0.990071\n",
      "1810 0.989856\n",
      "1820 0.989666\n",
      "1830 0.989482\n",
      "1840 0.989307\n",
      "1850 0.989187\n",
      "1860 0.988941\n",
      "1870 0.988776\n",
      "1880 0.98861\n",
      "1890 0.988496\n",
      "1900 0.988246\n",
      "1910 0.988076\n",
      "1920 0.987908\n",
      "1930 0.987741\n",
      "1940 0.987544\n",
      "1950 0.98738\n",
      "1960 0.987219\n",
      "1970 0.987058\n",
      "1980 0.986867\n",
      "1990 0.98671\n",
      "2000 0.986555\n",
      "2010 0.986369\n",
      "2020 0.986242\n",
      "2030 0.98605\n",
      "2040 0.985916\n",
      "2050 0.985768\n",
      "2060 0.985542\n",
      "2070 0.985383\n",
      "2080 0.985238\n",
      "2090 0.985065\n",
      "2100 0.984908\n",
      "2110 0.984786\n",
      "2120 0.984648\n",
      "2130 0.984418\n",
      "2140 0.984279\n",
      "2150 0.984139\n",
      "2160 0.983936\n",
      "2170 0.983795\n",
      "2180 0.983696\n",
      "2190 0.98347\n",
      "2200 0.983319\n",
      "2210 0.983241\n",
      "2220 0.983006\n",
      "2230 0.982857\n",
      "2240 0.982772\n",
      "2250 0.982544\n",
      "2260 0.982408\n",
      "2270 0.982291\n",
      "2280 0.982085\n",
      "2290 0.981964\n",
      "2300 0.981858\n",
      "2310 0.981655\n",
      "2320 0.981545\n",
      "2330 0.981346\n",
      "2340 0.981206\n",
      "2350 0.981072\n",
      "2360 0.980905\n",
      "2370 0.980773\n",
      "2380 0.980671\n",
      "2390 0.98048\n",
      "2400 0.98038\n",
      "2410 0.98022\n",
      "2420 0.980037\n",
      "2430 0.979947\n",
      "2440 0.979754\n",
      "2450 0.979661\n",
      "2460 0.97951\n",
      "2470 0.979329\n",
      "2480 0.979222\n",
      "2490 0.979061\n",
      "2500 0.978973\n",
      "2510 0.978771\n",
      "2520 0.978655\n",
      "2530 0.978483\n",
      "2540 0.978362\n",
      "2550 0.978283\n",
      "2560 0.978091\n",
      "2570 0.978011\n",
      "2580 0.977811\n",
      "2590 0.977709\n",
      "2600 0.977546\n",
      "2610 0.977413\n",
      "2620 0.977339\n",
      "2630 0.97715\n",
      "2640 0.977071\n",
      "2650 0.976893\n",
      "2660 0.976766\n",
      "2670 0.976611\n",
      "2680 0.976543\n",
      "2690 0.976352\n",
      "2700 0.97624\n",
      "2710 0.976105\n",
      "2720 0.975998\n",
      "2730 0.975876\n",
      "2740 0.975733\n",
      "2750 0.975588\n",
      "2760 0.975472\n",
      "2770 0.975329\n",
      "2780 0.975215\n",
      "2790 0.975075\n",
      "2800 0.974963\n",
      "2810 0.974825\n",
      "2820 0.974715\n",
      "2830 0.974579\n",
      "2840 0.974472\n",
      "2850 0.974338\n",
      "2860 0.974233\n",
      "2870 0.974125\n",
      "2880 0.973998\n",
      "2890 0.973853\n",
      "2900 0.973739\n",
      "2910 0.973601\n",
      "2920 0.97354\n",
      "2930 0.97336\n",
      "2940 0.973317\n",
      "2950 0.97314\n",
      "2960 0.973071\n",
      "2970 0.972898\n",
      "2980 0.97283\n",
      "2990 0.972657\n",
      "3000 0.972539\n",
      "예측값: [0 1 0 0 0 0]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 66.67\n"
     ]
    }
   ],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어본다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array([[0,0],[1,0],[1,1],[0,0],[0,0],[0,1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식은 Oh형식의 데이터라고 함 one-hot 데이터\n",
    "y_data = np.array([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [1,0,0],\n",
    "    [0,0,1]\n",
    "])\n",
    "\n",
    "#########================================\n",
    "# 신경망 모델 구성\n",
    "#########\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원으로 [입력층(특성), 출력층(레이블)] -> [2,3]으로 정한다.\n",
    "W = tf.Variable(tf.random_uniform([2,3],-1., 1.))\n",
    "\n",
    "# 편향을 각각 레이어의 아웃풋 갯수로 설정\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망에 가중치 W와 편향 b를 적용\n",
    "L = tf.add(tf.matmul(X,W),b)\n",
    "# 가중치와 편향을 이용해 계산한 결과값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU함수 적용\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# 마지막으로 softmax함수를 이용하여 출력값을 사용하기 쉽게 만든다.\n",
    "# softmax함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "model = tf.nn.softmax(L)\n",
    "\n",
    "# 신경망을 최적화하기 위한 비용 함수를 작성\n",
    "# 각 개별 결과에 대한 합을 구한뒤 평규을 내는 방식을 사용!\n",
    "# 전체 합이 아닌, 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션 사용\n",
    "# (axis 옵션이 없으면 -1.09처럼 총합인 스칼라값으로 출력됨)\n",
    "#        Y         model         Y * tf.log(model)   reduce_sum(axis=1)\n",
    "# 예) [[1 0 0]  [[0.1 0.7 0.2]  -> [[-1.0  0    0]  -> [-1.0, -0.09]\n",
    "#     [0 1 0]]  [0.2 0.8 0.0]]     [ 0   -0.09 0]]\n",
    "# 즉, 이것은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한 것이며,\n",
    "# 이것을 Cross-Entropy라고 한다.\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(model),axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########===============================\n",
    "# 신경망 모델 학습\n",
    "#########\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(3000): ## 이게 epoch와 같은건가? 100보다 30이 더 설득력있는데.. 그냥 트레이닝 스텝인듯.\n",
    "    sess.run(train_op, feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if (step+1)%10==0: # 학습상황을 프린트로 볼 수 있다.\n",
    "        print(step+1, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "        \n",
    "#########===============================\n",
    "# 결과확인\n",
    "# 0:기타 1:포유류 2:조류\n",
    "#########\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.argmax를 이용해 가장 큰 값을 가져온다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0]\n",
    "#    [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "\n",
    "prediction = tf.argmax(model,1)\n",
    "target = tf.argmax(Y,1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy*100, feed_dict={X:x_data, Y:y_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ◆ Tensorflow basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- 단순한 선형회귀모형을 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X_4:0\", dtype=float32)\n",
      "Tensor(\"Y_4:0\", dtype=float32)\n",
      "0 0.791301 [ 1.02837539] [ 0.03801347]\n",
      "1 0.00951704 [ 0.98668629] [ 0.01906063]\n",
      "2 0.000175426 [ 0.99148816] [ 0.02057398]\n",
      "3 6.09058e-05 [ 0.99120295] [ 0.01986391]\n",
      "4 5.67442e-05 [ 0.99146795] [ 0.01940995]\n",
      "5 5.40333e-05 [ 0.99166721] [ 0.01894079]\n",
      "6 5.14672e-05 [ 0.9918682] [ 0.01848576]\n",
      "7 4.90219e-05 [ 0.99206358] [ 0.01804134]\n",
      "8 4.66929e-05 [ 0.99225438] [ 0.01760764]\n",
      "9 4.44753e-05 [ 0.99244052] [ 0.01718434]\n",
      "10 4.23629e-05 [ 0.99262232] [ 0.01677127]\n",
      "11 4.03505e-05 [ 0.99279964] [ 0.01636809]\n",
      "12 3.84347e-05 [ 0.99297273] [ 0.01597461]\n",
      "13 3.66091e-05 [ 0.99314171] [ 0.01559062]\n",
      "14 3.48689e-05 [ 0.99330652] [ 0.01521581]\n",
      "15 3.32131e-05 [ 0.99346745] [ 0.01485004]\n",
      "16 3.16348e-05 [ 0.99362445] [ 0.01449303]\n",
      "17 3.01326e-05 [ 0.99377775] [ 0.01414466]\n",
      "18 2.87015e-05 [ 0.9939273] [ 0.01380462]\n",
      "19 2.73377e-05 [ 0.99407327] [ 0.01347278]\n",
      "20 2.604e-05 [ 0.99421579] [ 0.01314892]\n",
      "21 2.48024e-05 [ 0.99435478] [ 0.01283281]\n",
      "22 2.36242e-05 [ 0.9944905] [ 0.01252432]\n",
      "23 2.25028e-05 [ 0.99462301] [ 0.01222326]\n",
      "24 2.14337e-05 [ 0.99475223] [ 0.01192939]\n",
      "25 2.04152e-05 [ 0.99487841] [ 0.01164263]\n",
      "26 1.94449e-05 [ 0.99500149] [ 0.01136273]\n",
      "27 1.85218e-05 [ 0.99512166] [ 0.01108959]\n",
      "28 1.76425e-05 [ 0.99523896] [ 0.010823]\n",
      "29 1.68039e-05 [ 0.9953534] [ 0.01056283]\n",
      "30 1.60059e-05 [ 0.9954651] [ 0.01030891]\n",
      "31 1.52453e-05 [ 0.99557412] [ 0.01006108]\n",
      "32 1.4521e-05 [ 0.99568051] [ 0.00981922]\n",
      "33 1.38316e-05 [ 0.99578434] [ 0.00958318]\n",
      "34 1.31743e-05 [ 0.99588567] [ 0.0093528]\n",
      "35 1.25487e-05 [ 0.99598461] [ 0.00912798]\n",
      "36 1.19526e-05 [ 0.99608105] [ 0.00890853]\n",
      "37 1.13854e-05 [ 0.99617535] [ 0.00869439]\n",
      "38 1.08438e-05 [ 0.99626726] [ 0.00848538]\n",
      "39 1.03293e-05 [ 0.99635702] [ 0.0082814]\n",
      "40 9.8382e-06 [ 0.99644458] [ 0.00808231]\n",
      "41 9.37109e-06 [ 0.99653006] [ 0.00788801]\n",
      "42 8.92572e-06 [ 0.99661344] [ 0.00769838]\n",
      "43 8.50215e-06 [ 0.99669492] [ 0.00751334]\n",
      "44 8.09819e-06 [ 0.99677438] [ 0.00733272]\n",
      "45 7.71343e-06 [ 0.99685186] [ 0.00715643]\n",
      "46 7.34727e-06 [ 0.99692756] [ 0.0069844]\n",
      "47 6.99786e-06 [ 0.99700141] [ 0.00681649]\n",
      "48 6.66577e-06 [ 0.99707353] [ 0.00665264]\n",
      "49 6.3491e-06 [ 0.99714386] [ 0.00649272]\n",
      "50 6.0474e-06 [ 0.99721253] [ 0.00633663]\n",
      "51 5.76009e-06 [ 0.99727947] [ 0.00618427]\n",
      "52 5.48638e-06 [ 0.99734491] [ 0.00603563]\n",
      "53 5.22612e-06 [ 0.99740875] [ 0.00589054]\n",
      "54 4.97769e-06 [ 0.99747103] [ 0.00574894]\n",
      "55 4.74123e-06 [ 0.99753183] [ 0.00561074]\n",
      "56 4.51626e-06 [ 0.99759114] [ 0.00547586]\n",
      "57 4.30153e-06 [ 0.99764907] [ 0.00534424]\n",
      "58 4.09749e-06 [ 0.99770564] [ 0.00521578]\n",
      "59 3.90263e-06 [ 0.99776071] [ 0.00509036]\n",
      "60 3.71726e-06 [ 0.99781454] [ 0.004968]\n",
      "61 3.54092e-06 [ 0.99786717] [ 0.0048486]\n",
      "62 3.37242e-06 [ 0.99791837] [ 0.00473202]\n",
      "63 3.21219e-06 [ 0.99796838] [ 0.00461825]\n",
      "64 3.05981e-06 [ 0.99801731] [ 0.00450726]\n",
      "65 2.91439e-06 [ 0.99806488] [ 0.00439888]\n",
      "66 2.77597e-06 [ 0.99811149] [ 0.00429317]\n",
      "67 2.64409e-06 [ 0.99815685] [ 0.00418993]\n",
      "68 2.5183e-06 [ 0.99820113] [ 0.00408921]\n",
      "69 2.39892e-06 [ 0.9982444] [ 0.00399093]\n",
      "70 2.28475e-06 [ 0.9982866] [ 0.00389498]\n",
      "71 2.17634e-06 [ 0.99832779] [ 0.00380134]\n",
      "72 2.0728e-06 [ 0.99836797] [ 0.00370997]\n",
      "73 1.9745e-06 [ 0.99840724] [ 0.00362079]\n",
      "74 1.88071e-06 [ 0.99844551] [ 0.00353374]\n",
      "75 1.79131e-06 [ 0.99848282] [ 0.00344877]\n",
      "76 1.70617e-06 [ 0.99851936] [ 0.00336589]\n",
      "77 1.62526e-06 [ 0.99855494] [ 0.00328496]\n",
      "78 1.54801e-06 [ 0.99858969] [ 0.003206]\n",
      "79 1.47445e-06 [ 0.99862361] [ 0.00312893]\n",
      "80 1.40437e-06 [ 0.99865663] [ 0.0030537]\n",
      "81 1.33781e-06 [ 0.998689] [ 0.00298032]\n",
      "82 1.2742e-06 [ 0.99872047] [ 0.00290866]\n",
      "83 1.21359e-06 [ 0.99875122] [ 0.00283874]\n",
      "84 1.15599e-06 [ 0.9987812] [ 0.00277049]\n",
      "85 1.10121e-06 [ 0.99881059] [ 0.00270391]\n",
      "86 1.04886e-06 [ 0.99883914] [ 0.00263888]\n",
      "87 9.99065e-07 [ 0.99886709] [ 0.00257545]\n",
      "88 9.51543e-07 [ 0.99889427] [ 0.00251352]\n",
      "89 9.06312e-07 [ 0.99892092] [ 0.00245311]\n",
      "90 8.63259e-07 [ 0.99894685] [ 0.00239413]\n",
      "91 8.22258e-07 [ 0.99897212] [ 0.00233656]\n",
      "92 7.83279e-07 [ 0.99899685] [ 0.00228041]\n",
      "93 7.45884e-07 [ 0.99902093] [ 0.00222557]\n",
      "94 7.10563e-07 [ 0.99904448] [ 0.00217209]\n",
      "95 6.76818e-07 [ 0.99906749] [ 0.00211988]\n",
      "96 6.44714e-07 [ 0.9990899] [ 0.00206892]\n",
      "97 6.14049e-07 [ 0.99911177] [ 0.00201918]\n",
      "98 5.84823e-07 [ 0.99913311] [ 0.00197063]\n",
      "99 5.57137e-07 [ 0.99915397] [ 0.00192327]\n",
      "\n",
      "===Test===\n",
      "X: 5, Y: [ 4.99769306]\n",
      "X: 2.5, Y: [ 2.49980831]\n"
     ]
    }
   ],
   "source": [
    "# X와 Y의 상관관계를 분석하는 기초적인 선형회귀모델 만들기\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# name : 나중에 텐서보드등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙임\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "# 상관관계 분석을 위산 가설수실을 작성합니다.\n",
    "# y = W*x+b\n",
    "# W와 X가 행렬이 아님 ==> tf.matmul이 아닌 기본곱셈 사용\n",
    "hypothesis = W * X + b\n",
    "\n",
    "# 손실함수를 작성\n",
    "# mean(h-y)^2 : 예측값과 실제값의 거리를 비용함수로 정한다\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "# 텐서 플로우에 기본적으로 포함되어있는 함수를 이용, 경사하강법 최적화를 수행한다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# 비용을 최소화 하는것이 목표\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "# 세션을 생성하고 초기화\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 최적화를 100번 수행한다.\n",
    "    for step in range(100):\n",
    "        # sess.run을 통해 train_op와 cost그래프를 계산한다.\n",
    "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict을 통해 전달한다.\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y:y_data})\n",
    "        \n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "        \n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인\n",
    "    print(\"\\n===Test===\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X:5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable\n",
    "- 텐서플로우의 플레이스 홀더와 변수의 개념을 익히자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n",
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[-0.291262   -0.11953487]\n",
      " [-0.17635864  0.3196032 ]\n",
      " [ 0.96932673 -1.17966092]]\n",
      "=== b ===\n",
      "[[-0.78580576]\n",
      " [ 1.48390293]]\n",
      "===expr====\n",
      "[[ 1.47819519 -3.80511713]\n",
      " [ 5.25302219 -4.47418642]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#placeholder : 계산을 실행할 때 입력값을 받는 변수로 사용한다.\n",
    "#None은 크기가 정해지지 않았음을 의미함\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)\n",
    "\n",
    "# X플레이스홀더에 넣을값\n",
    "# 플레이스홀더에서 설정한 것처럼, 두번째 차원의 요소갯수는 3개\n",
    "x_data = [[1,2,3],[4,5,6]]\n",
    "\n",
    "#Variable : 그래프를 계산하면서 최적화 할 변수들. 이 값이 신경망을 좌우하는 값\n",
    "# random_normal : 각 변수들의 초기값을 정규분포랜덤값으로 초기화\n",
    "W = tf.Variable(tf.random_normal([3,2]))\n",
    "b = tf.Variable(tf.random_normal([2,1]))\n",
    "\n",
    "# 입력값과 변수들을 계산할 수식을 작성\n",
    "# tf.matmul 처럼 mat*로 되어있는 함수로 행렬계산을 수행\n",
    "expr = tf.matmul(X,W) + b\n",
    "\n",
    "sess = tf.Session()\n",
    "# 위에서 설정한 Variable들의 값을 초기화 해야됨\n",
    "# 처음에 tf.global_variables_initializer 를 한 번 실행해야함\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"===expr====\")\n",
    "print(sess.run(expr, feed_dict={X:x_data}))\n",
    "# expr 수식에는 X라는 입력값이 필요\n",
    "# 따라서 expr 실행시 이 변수에 대한 실제 입력값을 넣어줘야함\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic\n",
    "- 텐서플로우의 연산의 개념과 그래프를 실행하는 방법을 익힙니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_6:0\", shape=(), dtype=string)\n",
      "Tensor(\"Add_2:0\", shape=(), dtype=int32)\n",
      "b'Hello,TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# constant : 상수임\n",
    "hello = tf.constant('Hello,TensorFlow!')\n",
    "print(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a,b) # a+b로도 쓸 수 있다.\n",
    "print(c)\n",
    "\n",
    "# 위에서 변수와 수식들을 정의했지만, 실행이 정의한 시점에서 실행되는 것이 아님. 뭔가 스파크처럼 게으른 느낌\n",
    "# Session 객제와 run 메소드를 사용할 때 비로소 계산실행.\n",
    "# 따라서 모델을 구성하는 것과, 실행하는 것을 분리하여 프로그램을 깔끔하게 작성할 수 있음!!!\n",
    "# 그래프를 실행할 세션을 구성\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a,b,c]))\n",
    "\n",
    "#세션닫기\n",
    "sess.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
