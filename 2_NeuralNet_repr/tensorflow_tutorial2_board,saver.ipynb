{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png','retina'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다중출력여부\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우 튜토리얼 - 텐서보드 및 세이버\n",
    "# ※ 주의\n",
    "#### saver로 저장한 파라미터를 주피터에서 여러번 호출실행시 코드가 엉켜서 파라미터 에러가 난다. \n",
    "#### --> 이미 텐서플로가 실행되었기때문에 변수가 겹쳐서 일어나는 현상인듯. 한번 실행하면 리로드 필요.\n",
    "- 튜토리얼을 제공하는 깃허브 https://github.com/golbin/TensorFlow-Tutorials\n",
    "\n",
    "- 모두를 위한 딥러닝 유튜브강의 https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm\n",
    "\n",
    "- 텐서보드 실행법\n",
    "    1. tensorboard --logdir=./logs ###로그데이터의 위치로 지정\n",
    "    2. http://localhost:6006\n",
    "\n",
    "### 목차\n",
    "- saver - 트레이닝 상태 및 변수 저장하기/불러오기\n",
    "- TensorBoard - 시각화 설정하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saver\n",
    "- 모델을 저장하고 재사용하는 방법을 익히자\n",
    "- 학습상황을 저장하고 이후에 이 상황을 이어서 계속 학습을 시키는 등의 방법으로 사용할 수 있다.\n",
    "- checkpoint 저장 불러오기에 대한 자세한 설명 \n",
    "    - http://jaynewho.com/post/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1,  Cost: 1.542\n",
      "Step: 2,  Cost: 1.352\n",
      "예측값: [0 1 1 0 0 1]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 66.67\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt('./data.csv', delimiter=',',\n",
    "                 unpack=True, dtype='float32')\n",
    "\n",
    "# 털, 날개, 기타, 포유류, 조류\n",
    "# x_data = 0, 1\n",
    "# y_data = 2, 3, 4\n",
    "x_data = np.transpose(data[0:2])\n",
    "y_data = np.transpose(data[2:])\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "# 학습에 직접적으로 사용하지 않고 학습 횟수에 따라 단순히 증가시킬 변수를 만듭니다.\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "\n",
    "W2 = tf.Variable(tf.random_uniform([10, 20], -1., 1.))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2))\n",
    "\n",
    "W3 = tf.Variable(tf.random_uniform([20, 3], -1., 1.))\n",
    "model = tf.matmul(L2, W3)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "# global_step로 넘겨준 변수를, 학습용 변수들을 최적화 할 때 마다 학습 횟수를 하나씩 증가시킵니다.\n",
    "train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "sess = tf.Session()\n",
    "# 모델을 저장하고 불러오는 API를 초기화합니다.\n",
    "# global_variables 함수를 통해 앞서 정의하였던 변수들을 저장하거나 불러올 변수들로 설정합니다.\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('model')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 최적화 진행\n",
    "for step in range(2):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    print('Step: %d, ' % sess.run(global_step),\n",
    "          'Cost: %.3f' % sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "    \n",
    "# 최적화가 끝나고, 변수를 저장\n",
    "saver.save(sess, 'model/dnn.ckpt', global_step = global_step)\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "- 텐서보드를 이용하기 위해 각종 변수들을 설정하고 저장하는 방법\n",
    "- 실행법\n",
    "```\n",
    "# tensorboard --logdir=logs\n",
    "# http://localhost:6006\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 1, Cost: 0.898\n",
      "Step : 2, Cost: 0.872\n",
      "Step : 3, Cost: 0.851\n",
      "Step : 4, Cost: 0.830\n",
      "Step : 5, Cost: 0.813\n",
      "Step : 6, Cost: 0.796\n",
      "Step : 7, Cost: 0.780\n",
      "Step : 8, Cost: 0.765\n",
      "Step : 9, Cost: 0.751\n",
      "Step : 10, Cost: 0.737\n",
      "Step : 11, Cost: 0.724\n",
      "Step : 12, Cost: 0.711\n",
      "Step : 13, Cost: 0.699\n",
      "Step : 14, Cost: 0.687\n",
      "Step : 15, Cost: 0.676\n",
      "Step : 16, Cost: 0.665\n",
      "Step : 17, Cost: 0.654\n",
      "Step : 18, Cost: 0.644\n",
      "Step : 19, Cost: 0.634\n",
      "Step : 20, Cost: 0.626\n",
      "Step : 21, Cost: 0.618\n",
      "Step : 22, Cost: 0.610\n",
      "Step : 23, Cost: 0.604\n",
      "Step : 24, Cost: 0.598\n",
      "Step : 25, Cost: 0.593\n",
      "Step : 26, Cost: 0.588\n",
      "Step : 27, Cost: 0.583\n",
      "Step : 28, Cost: 0.579\n",
      "Step : 29, Cost: 0.576\n",
      "Step : 30, Cost: 0.572\n",
      "Step : 31, Cost: 0.570\n",
      "Step : 32, Cost: 0.567\n",
      "Step : 33, Cost: 0.565\n",
      "Step : 34, Cost: 0.563\n",
      "Step : 35, Cost: 0.562\n",
      "Step : 36, Cost: 0.561\n",
      "Step : 37, Cost: 0.560\n",
      "Step : 38, Cost: 0.558\n",
      "Step : 39, Cost: 0.558\n",
      "Step : 40, Cost: 0.557\n",
      "Step : 41, Cost: 0.556\n",
      "Step : 42, Cost: 0.555\n",
      "Step : 43, Cost: 0.555\n",
      "Step : 44, Cost: 0.554\n",
      "Step : 45, Cost: 0.554\n",
      "Step : 46, Cost: 0.554\n",
      "Step : 47, Cost: 0.553\n",
      "Step : 48, Cost: 0.553\n",
      "Step : 49, Cost: 0.553\n",
      "Step : 50, Cost: 0.552\n",
      "Step : 51, Cost: 0.552\n",
      "Step : 52, Cost: 0.552\n",
      "Step : 53, Cost: 0.552\n",
      "Step : 54, Cost: 0.552\n",
      "Step : 55, Cost: 0.552\n",
      "Step : 56, Cost: 0.551\n",
      "Step : 57, Cost: 0.551\n",
      "Step : 58, Cost: 0.551\n",
      "Step : 59, Cost: 0.551\n",
      "Step : 60, Cost: 0.551\n",
      "Step : 61, Cost: 0.551\n",
      "Step : 62, Cost: 0.551\n",
      "Step : 63, Cost: 0.551\n",
      "Step : 64, Cost: 0.551\n",
      "Step : 65, Cost: 0.551\n",
      "Step : 66, Cost: 0.551\n",
      "Step : 67, Cost: 0.551\n",
      "Step : 68, Cost: 0.551\n",
      "Step : 69, Cost: 0.551\n",
      "Step : 70, Cost: 0.551\n",
      "Step : 71, Cost: 0.551\n",
      "Step : 72, Cost: 0.551\n",
      "Step : 73, Cost: 0.550\n",
      "Step : 74, Cost: 0.550\n",
      "Step : 75, Cost: 0.550\n",
      "Step : 76, Cost: 0.550\n",
      "Step : 77, Cost: 0.550\n",
      "Step : 78, Cost: 0.550\n",
      "Step : 79, Cost: 0.550\n",
      "Step : 80, Cost: 0.550\n",
      "Step : 81, Cost: 0.550\n",
      "Step : 82, Cost: 0.550\n",
      "Step : 83, Cost: 0.550\n",
      "Step : 84, Cost: 0.550\n",
      "Step : 85, Cost: 0.550\n",
      "Step : 86, Cost: 0.550\n",
      "Step : 87, Cost: 0.550\n",
      "Step : 88, Cost: 0.550\n",
      "Step : 89, Cost: 0.550\n",
      "Step : 90, Cost: 0.550\n",
      "Step : 91, Cost: 0.550\n",
      "Step : 92, Cost: 0.550\n",
      "Step : 93, Cost: 0.550\n",
      "Step : 94, Cost: 0.550\n",
      "Step : 95, Cost: 0.550\n",
      "Step : 96, Cost: 0.550\n",
      "Step : 97, Cost: 0.550\n",
      "Step : 98, Cost: 0.550\n",
      "Step : 99, Cost: 0.550\n",
      "Step : 100, Cost: 0.550\n",
      "prediction value :  [0 1 2 0 0 2]\n",
      "original value :  [0 1 2 0 0 2]\n",
      "accuracy : 100.00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt('./data.csv', delimiter=',',\n",
    "                 unpack=True, dtype='float32')\n",
    "\n",
    "x_data = np.transpose(data[0:2])\n",
    "y_data = np.transpose(data[2:])\n",
    "\n",
    "#########====================\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# ★★★with tf.name_scope 으로 묶은 블럭은 텐서보드에서 한 레이어안에 표시됨----------\n",
    "with tf.name_scope('layer1'):\n",
    "    W1 = tf.Variable(tf.random_uniform([2,10],-1.,1.),name='W1')\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "    \n",
    "    tf.summary.histogram(\"X\", X)  # ★★★★히스토그램 추가하기----------------\n",
    "    tf.summary.histogram(\"Weights\", W1)\n",
    "\n",
    "with tf.name_scope('layer2'):\n",
    "    W2 = tf.Variable(tf.random_uniform([10,20],-1.,1.),name='W2')\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2))\n",
    "    \n",
    "    tf.summary.histogram(\"Weights\", W2)\n",
    "\n",
    "\n",
    "with tf.name_scope('output'):\n",
    "    W3 = tf.Variable(tf.random_uniform([20,3],-1.,1.),name='W3')\n",
    "    model = tf.matmul(L2, W3)\n",
    "    \n",
    "    tf.summary.histogram(\"Weights\", W3)\n",
    "    tf.summary.histogram(\"Model\", model)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    \n",
    "    # tf.summary.scalar를 이용해 수집하고 싶은 값들을 지정할 수 있음\n",
    "    tf.summary.scalar('cost',cost)\n",
    "\n",
    "#########======================\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('model')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "# 텐서보드에 표시하기 위한 텐서들을 수집하는 작업\n",
    "merged = tf.summary.merge_all()\n",
    "# 저장할 그래프와 텐서값들을 저장할 디렉토리를 설정한다.\n",
    "writer = tf.summary.FileWriter('logs',sess.graph)\n",
    "\n",
    "# 이렇게 저장한 로그는, 학습 후 다음의 명령어를 이용해 웹서버를 실행시킨 뒤\n",
    "# tensorboard --logdir=./logs\n",
    "# 다음 주소와 웹브라우저를 이용해 텐서보드에서 확인할 수 있습니다.\n",
    "# http://localhost:6006\n",
    "\n",
    "# 최적화 진행\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    print('Step : %d,' % sess.run(global_step),\n",
    "         'Cost: %.3f' % sess.run(cost,feed_dict={X:x_data, Y:y_data}))\n",
    "    \n",
    "    #적절한 시점에 저장할 값들을 수집하고 저장\n",
    "    summary = sess.run(merged, feed_dict={X:x_data, Y:y_data})\n",
    "    writer.add_summary(summary, global_step=sess.run(global_step))\n",
    "    \n",
    "saver.save(sess, 'model/dnn.ckpt', global_step=global_step)\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y,1)\n",
    "print('prediction value : ', sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('original value : ', sess.run(target, feed_dict={Y:y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('accuracy : %.2f' % sess.run(accuracy*100, feed_dict={X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 자연어 분석에 매우 중요하개 사용되는 Word2Vec모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#한글폰트 적용\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "#음수처리\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 단어 벡터를 분석해볼 임의의 문장\n",
    "sentences = [\"나 고양이 좋다\",\n",
    "             \"나 강아지 좋다\",\n",
    "             \"나 동물 좋다\",\n",
    "             \"강아지 고양이 동물\",\n",
    "             \"여자친구 고양이 강아지 좋다\",\n",
    "             \"고양이 생선 우유 좋다\",\n",
    "             \"강아지 생선 싫다 우유 좋다\",\n",
    "             \"강아지 고양이 눈 좋다\",\n",
    "             \"나 여자친구 좋다\",\n",
    "             \"여자친구 나 싫다\",\n",
    "             \"여자친구 나 영화 책 음악 좋다\",\n",
    "             \"나 게임 만화 애니 좋다\",\n",
    "             \"고양이 강아지 싫다\",\n",
    "             \"강아지 고양이 좋다\"]\n",
    "\n",
    "# 문장을 전부 합친 후 공백으로 단어들을 나누고 고유한 단어들로 리스트를 만든다.\n",
    "word_sequence = \" \".join(sentences).split()\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "# 문자열로 분석하는 것 보다, 숫자로 분석하는 것이 훨씬 용이함\n",
    "# 리스트에서 문자들의 인덱스를 뽑아 사용하기 위해서\n",
    "# 이를 표현하기 위한 연관 배열과, 단어 리스트에서 단어를 참조할 수 있는 인덱스 배열을 만든다\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "# 윈도우 사이즈를 1로하는 skip-gram모델을 만든다\n",
    "# 예) 나 게임 만화 애니 좋다\n",
    "#   -> ([나, 만화], 게임), ([게임, 애니], 만화), ([만화, 좋다], 애니)\n",
    "#   -> (게임, 나), (게임, 만화), (만화, 게임), (만화, 애니), (애니, 만화), (애니, 좋다)\n",
    "skip_grams=[]\n",
    "\n",
    "for i in range(1, len(word_sequence)-1):\n",
    "    # (context, target) : ([target index -1, target_index +1], target)\n",
    "    # 스킵그램을 만든 후, 저장은 단어의 고류 번호(index)로 저장\n",
    "    target = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i-1]],word_dict[word_sequence[i+1]]]\n",
    "    \n",
    "    # (target, context[0]),(target, context[1])...\n",
    "    for w in context:\n",
    "        skip_grams.append([target, w])\n",
    "\n",
    "# skip-gram 데이터에서 무작위로 데이터를 뽑아 입력값과 출력값의 배치 데이터를 생성하는 함수\n",
    "def random_batch(data,size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)),size, replace=False)\n",
    "    \n",
    "    for i in random_index:\n",
    "        random_inputs.append(data[i][0]) # target\n",
    "        random_labels.append([data[i][1]]) #context word\n",
    "        \n",
    "    return random_inputs, random_labels\n",
    "\n",
    "#########=======================================\n",
    "# 옵션 설정\n",
    "######\n",
    "# 학습을 반복할 횟수\n",
    "training_epoch = 1000\n",
    "# 학습률\n",
    "learning_rate = 0.1\n",
    "# 한번에 학습할 데이터의 크기\n",
    "batch_size = 20\n",
    "# 단어 벡터를 구성할 임베딩 차원의 크기\n",
    "# 이 예제는 x,y그래프로 표현하기 쉽도록 2개값만 출력\n",
    "embedding_size = 2\n",
    "# word2ved 모델 학습을 위한 nce_loss 함수에서 사용하기 위한 샘플링 크기\n",
    "# batch_size 보다 작아야 한다.\n",
    "num_sampled = 15\n",
    "# 총 단어갯수\n",
    "voc_size = len(word_list)\n",
    "\n",
    "#########=======================================\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# tf.nn.nce_loss를 사용하려면 출력값을 이렇게 [batch_size, 1] 구성해야한다.\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# word2vec 모델의 결과값인 임베딩 벡터를 저장할 변수다.\n",
    "# 총 단어 갯수와 임베딩 갯수를 크기로 하는 두개의 차원을 갖는다.\n",
    "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size],-1.0, 1.0))\n",
    "# 임베팅 벡터의 차원에서 학습할 입력값에 대한 행들을 뽑아온다\n",
    "# 예) embeddings     inputs    selected\n",
    "#    [[1, 2, 3]  -> [2, 3] -> [[2, 3, 4]\n",
    "#     [2, 3, 4]                [3, 4, 5]]\n",
    "#     [3, 4, 5]\n",
    "#     [4, 5, 6]]\n",
    "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# nce_loss 함수에서 사용할 변수들을 정의한다.\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "# nce_loss 함수를 직접 구현하려면 매우 복잡하지만,\n",
    "# 텐서플로우에서 제공하는 함수가 있으니 tf.nn.nce_loss함수를 사용하면됨\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "#########==========================================\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init= tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, training_epoch + 1):\n",
    "        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
    "        \n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                              feed_dict={inputs:batch_inputs,\n",
    "                                         labels:batch_labels})\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss at step \",step, \": \",loss_val)\n",
    "    \n",
    "    # matplot 으로 출력하여 시각적으로 확인해보기 위해\n",
    "    # 임베딩 벡터의 결과 값을 계한하여 저장한다.\n",
    "    # with 구문 안에서는 sess.run 대신 간단히 eval() 함수를 사용할 수 있다.\n",
    "    trained_embeddings = embeddings.eval()\n",
    "    \n",
    "#########=========================================\n",
    "# 임베딩된 Word2Vec 결과 확인\n",
    "# 결과는 해당 단어들이 얼마나 다른 단어와 인접해 있는지를 보여줍니다.\n",
    "######    \n",
    "for i, label in enumerate(word_list):\n",
    "    x,y = trained_embeddings[i]\n",
    "    plt.scatter(x,y)\n",
    "    plt.annotate(label, xy=(x,y), xytext=(5,2),\n",
    "                textcoords='offset points', ha='right', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN\n",
    "- 여러개의 신경망을 구성하는 방법 익혀보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.10388\n",
      "20 0.866431\n",
      "30 0.674503\n",
      "40 0.533089\n",
      "50 0.42709\n",
      "60 0.348081\n",
      "70 0.287232\n",
      "80 0.238058\n",
      "90 0.197017\n",
      "100 0.163432\n",
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "# 신경망의 레이어를 여러개로 구성하여 말로만 듣던 딥러닝을 구성해 봅시다!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########================================\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2,10]으로 정한다.\n",
    "W1 = tf.Variable(tf.random_uniform([2,10], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류갯수] -> [10,3] 으로 정한다.\n",
    "W2 = tf.Variable(tf.random_uniform([10,3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정한다.\n",
    "# b1은 히든 레이어의 뉴런 갯수로, b2는 최종 결과값 즉, 분류갯수인 3으로 설정\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용\n",
    "L1 = tf.add(tf.matmul(X, W1),b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산한다\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어낸다\n",
    "model = tf.add(tf.matmul(L1, W2),b2)\n",
    "\n",
    "# 덴서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용함수를 다음처럼 간단하게 적용할 수 있음\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########================================\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if (step + 1)% 10 == 0:\n",
    "        print(step+1, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "        \n",
    "        \n",
    "\n",
    "#########================================\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model,1)\n",
    "target = tf.arg_max(Y,1)\n",
    "prediction = tf.argmax(model,1)\n",
    "target = tf.argmax(Y,1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy*100, feed_dict={X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "- 신경망을 구성하여 간단한 분류 모델을 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.04917\n",
      "20 1.04776\n",
      "30 1.0465\n",
      "40 1.04516\n",
      "50 1.04391\n",
      "60 1.04267\n",
      "70 1.04139\n",
      "80 1.0402\n",
      "90 1.03936\n",
      "100 1.03854\n",
      "110 1.03775\n",
      "120 1.03698\n",
      "130 1.03619\n",
      "140 1.03545\n",
      "150 1.03467\n",
      "160 1.034\n",
      "170 1.03321\n",
      "180 1.0325\n",
      "190 1.0318\n",
      "200 1.03108\n",
      "210 1.0304\n",
      "220 1.02977\n",
      "230 1.02942\n",
      "240 1.02909\n",
      "250 1.02873\n",
      "260 1.02849\n",
      "270 1.02806\n",
      "280 1.02775\n",
      "290 1.0274\n",
      "300 1.02708\n",
      "310 1.02674\n",
      "320 1.02645\n",
      "330 1.02609\n",
      "340 1.02575\n",
      "350 1.02546\n",
      "360 1.02511\n",
      "370 1.02477\n",
      "380 1.02448\n",
      "390 1.02414\n",
      "400 1.02391\n",
      "410 1.02351\n",
      "420 1.02318\n",
      "430 1.0229\n",
      "440 1.02257\n",
      "450 1.02233\n",
      "460 1.02196\n",
      "470 1.02163\n",
      "480 1.02136\n",
      "490 1.02103\n",
      "500 1.02072\n",
      "510 1.02045\n",
      "520 1.02012\n",
      "530 1.01981\n",
      "540 1.01955\n",
      "550 1.01922\n",
      "560 1.01891\n",
      "570 1.01868\n",
      "580 1.01834\n",
      "590 1.01803\n",
      "600 1.01772\n",
      "610 1.01752\n",
      "620 1.01716\n",
      "630 1.01687\n",
      "640 1.01661\n",
      "650 1.01631\n",
      "660 1.01601\n",
      "670 1.01577\n",
      "680 1.01552\n",
      "690 1.01518\n",
      "700 1.01489\n",
      "710 1.01464\n",
      "720 1.01436\n",
      "730 1.01408\n",
      "740 1.01377\n",
      "750 1.01349\n",
      "760 1.01329\n",
      "770 1.01296\n",
      "780 1.01268\n",
      "790 1.01241\n",
      "800 1.01219\n",
      "810 1.01193\n",
      "820 1.01162\n",
      "830 1.01134\n",
      "840 1.01107\n",
      "850 1.0108\n",
      "860 1.01053\n",
      "870 1.01037\n",
      "880 1.01004\n",
      "890 1.00976\n",
      "900 1.00951\n",
      "910 1.00924\n",
      "920 1.00898\n",
      "930 1.00876\n",
      "940 1.00856\n",
      "950 1.00826\n",
      "960 1.00798\n",
      "970 1.00773\n",
      "980 1.00747\n",
      "990 1.00724\n",
      "1000 1.00696\n",
      "1010 1.00675\n",
      "1020 1.00648\n",
      "1030 1.00625\n",
      "1040 1.00602\n",
      "1050 1.00575\n",
      "1060 1.00551\n",
      "1070 1.00527\n",
      "1080 1.00502\n",
      "1090 1.00478\n",
      "1100 1.0046\n",
      "1110 1.0043\n",
      "1120 1.00407\n",
      "1130 1.0039\n",
      "1140 1.00365\n",
      "1150 1.00346\n",
      "1160 1.00322\n",
      "1170 1.00292\n",
      "1180 1.00269\n",
      "1190 1.00249\n",
      "1200 1.00225\n",
      "1210 1.00202\n",
      "1220 1.00178\n",
      "1230 1.00155\n",
      "1240 1.00132\n",
      "1250 1.00109\n",
      "1260 1.00087\n",
      "1270 1.00064\n",
      "1280 1.00042\n",
      "1290 1.00021\n",
      "1300 0.999989\n",
      "1310 0.999774\n",
      "1320 0.999562\n",
      "1330 0.999352\n",
      "1340 0.999144\n",
      "1350 0.998939\n",
      "1360 0.998687\n",
      "1370 0.998471\n",
      "1380 0.998334\n",
      "1390 0.998121\n",
      "1400 0.99788\n",
      "1410 0.997676\n",
      "1420 0.997392\n",
      "1430 0.997188\n",
      "1440 0.997013\n",
      "1450 0.996775\n",
      "1460 0.996565\n",
      "1470 0.996377\n",
      "1480 0.996168\n",
      "1490 0.99596\n",
      "1500 0.995774\n",
      "1510 0.995604\n",
      "1520 0.995362\n",
      "1530 0.99521\n",
      "1540 0.994948\n",
      "1550 0.994798\n",
      "1560 0.994548\n",
      "1570 0.994359\n",
      "1580 0.994163\n",
      "1590 0.993971\n",
      "1600 0.993815\n",
      "1610 0.99366\n",
      "1620 0.993438\n",
      "1630 0.993184\n",
      "1640 0.992987\n",
      "1650 0.992811\n",
      "1660 0.992611\n",
      "1670 0.992461\n",
      "1680 0.992319\n",
      "1690 0.99211\n",
      "1700 0.991879\n",
      "1710 0.991664\n",
      "1720 0.991484\n",
      "1730 0.991328\n",
      "1740 0.991196\n",
      "1750 0.990999\n",
      "1760 0.990741\n",
      "1770 0.99057\n",
      "1780 0.990383\n",
      "1790 0.990205\n",
      "1800 0.990071\n",
      "1810 0.989856\n",
      "1820 0.989666\n",
      "1830 0.989482\n",
      "1840 0.989307\n",
      "1850 0.989187\n",
      "1860 0.988941\n",
      "1870 0.988776\n",
      "1880 0.98861\n",
      "1890 0.988496\n",
      "1900 0.988246\n",
      "1910 0.988076\n",
      "1920 0.987908\n",
      "1930 0.987741\n",
      "1940 0.987544\n",
      "1950 0.98738\n",
      "1960 0.987219\n",
      "1970 0.987058\n",
      "1980 0.986867\n",
      "1990 0.98671\n",
      "2000 0.986555\n",
      "2010 0.986369\n",
      "2020 0.986242\n",
      "2030 0.98605\n",
      "2040 0.985916\n",
      "2050 0.985768\n",
      "2060 0.985542\n",
      "2070 0.985383\n",
      "2080 0.985238\n",
      "2090 0.985065\n",
      "2100 0.984908\n",
      "2110 0.984786\n",
      "2120 0.984648\n",
      "2130 0.984418\n",
      "2140 0.984279\n",
      "2150 0.984139\n",
      "2160 0.983936\n",
      "2170 0.983795\n",
      "2180 0.983696\n",
      "2190 0.98347\n",
      "2200 0.983319\n",
      "2210 0.983241\n",
      "2220 0.983006\n",
      "2230 0.982857\n",
      "2240 0.982772\n",
      "2250 0.982544\n",
      "2260 0.982408\n",
      "2270 0.982291\n",
      "2280 0.982085\n",
      "2290 0.981964\n",
      "2300 0.981858\n",
      "2310 0.981655\n",
      "2320 0.981545\n",
      "2330 0.981346\n",
      "2340 0.981206\n",
      "2350 0.981072\n",
      "2360 0.980905\n",
      "2370 0.980773\n",
      "2380 0.980671\n",
      "2390 0.98048\n",
      "2400 0.98038\n",
      "2410 0.98022\n",
      "2420 0.980037\n",
      "2430 0.979947\n",
      "2440 0.979754\n",
      "2450 0.979661\n",
      "2460 0.97951\n",
      "2470 0.979329\n",
      "2480 0.979222\n",
      "2490 0.979061\n",
      "2500 0.978973\n",
      "2510 0.978771\n",
      "2520 0.978655\n",
      "2530 0.978483\n",
      "2540 0.978362\n",
      "2550 0.978283\n",
      "2560 0.978091\n",
      "2570 0.978011\n",
      "2580 0.977811\n",
      "2590 0.977709\n",
      "2600 0.977546\n",
      "2610 0.977413\n",
      "2620 0.977339\n",
      "2630 0.97715\n",
      "2640 0.977071\n",
      "2650 0.976893\n",
      "2660 0.976766\n",
      "2670 0.976611\n",
      "2680 0.976543\n",
      "2690 0.976352\n",
      "2700 0.97624\n",
      "2710 0.976105\n",
      "2720 0.975998\n",
      "2730 0.975876\n",
      "2740 0.975733\n",
      "2750 0.975588\n",
      "2760 0.975472\n",
      "2770 0.975329\n",
      "2780 0.975215\n",
      "2790 0.975075\n",
      "2800 0.974963\n",
      "2810 0.974825\n",
      "2820 0.974715\n",
      "2830 0.974579\n",
      "2840 0.974472\n",
      "2850 0.974338\n",
      "2860 0.974233\n",
      "2870 0.974125\n",
      "2880 0.973998\n",
      "2890 0.973853\n",
      "2900 0.973739\n",
      "2910 0.973601\n",
      "2920 0.97354\n",
      "2930 0.97336\n",
      "2940 0.973317\n",
      "2950 0.97314\n",
      "2960 0.973071\n",
      "2970 0.972898\n",
      "2980 0.97283\n",
      "2990 0.972657\n",
      "3000 0.972539\n",
      "예측값: [0 1 0 0 0 0]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 66.67\n"
     ]
    }
   ],
   "source": [
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어본다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array([[0,0],[1,0],[1,1],[0,0],[0,0],[0,1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식은 Oh형식의 데이터라고 함 one-hot 데이터\n",
    "y_data = np.array([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [1,0,0],\n",
    "    [0,0,1]\n",
    "])\n",
    "\n",
    "#########================================\n",
    "# 신경망 모델 구성\n",
    "#########\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원으로 [입력층(특성), 출력층(레이블)] -> [2,3]으로 정한다.\n",
    "W = tf.Variable(tf.random_uniform([2,3],-1., 1.))\n",
    "\n",
    "# 편향을 각각 레이어의 아웃풋 갯수로 설정\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망에 가중치 W와 편향 b를 적용\n",
    "L = tf.add(tf.matmul(X,W),b)\n",
    "# 가중치와 편향을 이용해 계산한 결과값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU함수 적용\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# 마지막으로 softmax함수를 이용하여 출력값을 사용하기 쉽게 만든다.\n",
    "# softmax함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "model = tf.nn.softmax(L)\n",
    "\n",
    "# 신경망을 최적화하기 위한 비용 함수를 작성\n",
    "# 각 개별 결과에 대한 합을 구한뒤 평규을 내는 방식을 사용!\n",
    "# 전체 합이 아닌, 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션 사용\n",
    "# (axis 옵션이 없으면 -1.09처럼 총합인 스칼라값으로 출력됨)\n",
    "#        Y         model         Y * tf.log(model)   reduce_sum(axis=1)\n",
    "# 예) [[1 0 0]  [[0.1 0.7 0.2]  -> [[-1.0  0    0]  -> [-1.0, -0.09]\n",
    "#     [0 1 0]]  [0.2 0.8 0.0]]     [ 0   -0.09 0]]\n",
    "# 즉, 이것은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한 것이며,\n",
    "# 이것을 Cross-Entropy라고 한다.\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(model),axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########===============================\n",
    "# 신경망 모델 학습\n",
    "#########\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(3000): ## 이게 epoch와 같은건가? 100보다 30이 더 설득력있는데.. 그냥 트레이닝 스텝인듯.\n",
    "    sess.run(train_op, feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if (step+1)%10==0: # 학습상황을 프린트로 볼 수 있다.\n",
    "        print(step+1, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "        \n",
    "#########===============================\n",
    "# 결과확인\n",
    "# 0:기타 1:포유류 2:조류\n",
    "#########\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.argmax를 이용해 가장 큰 값을 가져온다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0]\n",
    "#    [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "\n",
    "prediction = tf.argmax(model,1)\n",
    "target = tf.argmax(Y,1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy*100, feed_dict={X:x_data, Y:y_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ◆ Tensorflow basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- 단순한 선형회귀모형을 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X_4:0\", dtype=float32)\n",
      "Tensor(\"Y_4:0\", dtype=float32)\n",
      "0 0.791301 [ 1.02837539] [ 0.03801347]\n",
      "1 0.00951704 [ 0.98668629] [ 0.01906063]\n",
      "2 0.000175426 [ 0.99148816] [ 0.02057398]\n",
      "3 6.09058e-05 [ 0.99120295] [ 0.01986391]\n",
      "4 5.67442e-05 [ 0.99146795] [ 0.01940995]\n",
      "5 5.40333e-05 [ 0.99166721] [ 0.01894079]\n",
      "6 5.14672e-05 [ 0.9918682] [ 0.01848576]\n",
      "7 4.90219e-05 [ 0.99206358] [ 0.01804134]\n",
      "8 4.66929e-05 [ 0.99225438] [ 0.01760764]\n",
      "9 4.44753e-05 [ 0.99244052] [ 0.01718434]\n",
      "10 4.23629e-05 [ 0.99262232] [ 0.01677127]\n",
      "11 4.03505e-05 [ 0.99279964] [ 0.01636809]\n",
      "12 3.84347e-05 [ 0.99297273] [ 0.01597461]\n",
      "13 3.66091e-05 [ 0.99314171] [ 0.01559062]\n",
      "14 3.48689e-05 [ 0.99330652] [ 0.01521581]\n",
      "15 3.32131e-05 [ 0.99346745] [ 0.01485004]\n",
      "16 3.16348e-05 [ 0.99362445] [ 0.01449303]\n",
      "17 3.01326e-05 [ 0.99377775] [ 0.01414466]\n",
      "18 2.87015e-05 [ 0.9939273] [ 0.01380462]\n",
      "19 2.73377e-05 [ 0.99407327] [ 0.01347278]\n",
      "20 2.604e-05 [ 0.99421579] [ 0.01314892]\n",
      "21 2.48024e-05 [ 0.99435478] [ 0.01283281]\n",
      "22 2.36242e-05 [ 0.9944905] [ 0.01252432]\n",
      "23 2.25028e-05 [ 0.99462301] [ 0.01222326]\n",
      "24 2.14337e-05 [ 0.99475223] [ 0.01192939]\n",
      "25 2.04152e-05 [ 0.99487841] [ 0.01164263]\n",
      "26 1.94449e-05 [ 0.99500149] [ 0.01136273]\n",
      "27 1.85218e-05 [ 0.99512166] [ 0.01108959]\n",
      "28 1.76425e-05 [ 0.99523896] [ 0.010823]\n",
      "29 1.68039e-05 [ 0.9953534] [ 0.01056283]\n",
      "30 1.60059e-05 [ 0.9954651] [ 0.01030891]\n",
      "31 1.52453e-05 [ 0.99557412] [ 0.01006108]\n",
      "32 1.4521e-05 [ 0.99568051] [ 0.00981922]\n",
      "33 1.38316e-05 [ 0.99578434] [ 0.00958318]\n",
      "34 1.31743e-05 [ 0.99588567] [ 0.0093528]\n",
      "35 1.25487e-05 [ 0.99598461] [ 0.00912798]\n",
      "36 1.19526e-05 [ 0.99608105] [ 0.00890853]\n",
      "37 1.13854e-05 [ 0.99617535] [ 0.00869439]\n",
      "38 1.08438e-05 [ 0.99626726] [ 0.00848538]\n",
      "39 1.03293e-05 [ 0.99635702] [ 0.0082814]\n",
      "40 9.8382e-06 [ 0.99644458] [ 0.00808231]\n",
      "41 9.37109e-06 [ 0.99653006] [ 0.00788801]\n",
      "42 8.92572e-06 [ 0.99661344] [ 0.00769838]\n",
      "43 8.50215e-06 [ 0.99669492] [ 0.00751334]\n",
      "44 8.09819e-06 [ 0.99677438] [ 0.00733272]\n",
      "45 7.71343e-06 [ 0.99685186] [ 0.00715643]\n",
      "46 7.34727e-06 [ 0.99692756] [ 0.0069844]\n",
      "47 6.99786e-06 [ 0.99700141] [ 0.00681649]\n",
      "48 6.66577e-06 [ 0.99707353] [ 0.00665264]\n",
      "49 6.3491e-06 [ 0.99714386] [ 0.00649272]\n",
      "50 6.0474e-06 [ 0.99721253] [ 0.00633663]\n",
      "51 5.76009e-06 [ 0.99727947] [ 0.00618427]\n",
      "52 5.48638e-06 [ 0.99734491] [ 0.00603563]\n",
      "53 5.22612e-06 [ 0.99740875] [ 0.00589054]\n",
      "54 4.97769e-06 [ 0.99747103] [ 0.00574894]\n",
      "55 4.74123e-06 [ 0.99753183] [ 0.00561074]\n",
      "56 4.51626e-06 [ 0.99759114] [ 0.00547586]\n",
      "57 4.30153e-06 [ 0.99764907] [ 0.00534424]\n",
      "58 4.09749e-06 [ 0.99770564] [ 0.00521578]\n",
      "59 3.90263e-06 [ 0.99776071] [ 0.00509036]\n",
      "60 3.71726e-06 [ 0.99781454] [ 0.004968]\n",
      "61 3.54092e-06 [ 0.99786717] [ 0.0048486]\n",
      "62 3.37242e-06 [ 0.99791837] [ 0.00473202]\n",
      "63 3.21219e-06 [ 0.99796838] [ 0.00461825]\n",
      "64 3.05981e-06 [ 0.99801731] [ 0.00450726]\n",
      "65 2.91439e-06 [ 0.99806488] [ 0.00439888]\n",
      "66 2.77597e-06 [ 0.99811149] [ 0.00429317]\n",
      "67 2.64409e-06 [ 0.99815685] [ 0.00418993]\n",
      "68 2.5183e-06 [ 0.99820113] [ 0.00408921]\n",
      "69 2.39892e-06 [ 0.9982444] [ 0.00399093]\n",
      "70 2.28475e-06 [ 0.9982866] [ 0.00389498]\n",
      "71 2.17634e-06 [ 0.99832779] [ 0.00380134]\n",
      "72 2.0728e-06 [ 0.99836797] [ 0.00370997]\n",
      "73 1.9745e-06 [ 0.99840724] [ 0.00362079]\n",
      "74 1.88071e-06 [ 0.99844551] [ 0.00353374]\n",
      "75 1.79131e-06 [ 0.99848282] [ 0.00344877]\n",
      "76 1.70617e-06 [ 0.99851936] [ 0.00336589]\n",
      "77 1.62526e-06 [ 0.99855494] [ 0.00328496]\n",
      "78 1.54801e-06 [ 0.99858969] [ 0.003206]\n",
      "79 1.47445e-06 [ 0.99862361] [ 0.00312893]\n",
      "80 1.40437e-06 [ 0.99865663] [ 0.0030537]\n",
      "81 1.33781e-06 [ 0.998689] [ 0.00298032]\n",
      "82 1.2742e-06 [ 0.99872047] [ 0.00290866]\n",
      "83 1.21359e-06 [ 0.99875122] [ 0.00283874]\n",
      "84 1.15599e-06 [ 0.9987812] [ 0.00277049]\n",
      "85 1.10121e-06 [ 0.99881059] [ 0.00270391]\n",
      "86 1.04886e-06 [ 0.99883914] [ 0.00263888]\n",
      "87 9.99065e-07 [ 0.99886709] [ 0.00257545]\n",
      "88 9.51543e-07 [ 0.99889427] [ 0.00251352]\n",
      "89 9.06312e-07 [ 0.99892092] [ 0.00245311]\n",
      "90 8.63259e-07 [ 0.99894685] [ 0.00239413]\n",
      "91 8.22258e-07 [ 0.99897212] [ 0.00233656]\n",
      "92 7.83279e-07 [ 0.99899685] [ 0.00228041]\n",
      "93 7.45884e-07 [ 0.99902093] [ 0.00222557]\n",
      "94 7.10563e-07 [ 0.99904448] [ 0.00217209]\n",
      "95 6.76818e-07 [ 0.99906749] [ 0.00211988]\n",
      "96 6.44714e-07 [ 0.9990899] [ 0.00206892]\n",
      "97 6.14049e-07 [ 0.99911177] [ 0.00201918]\n",
      "98 5.84823e-07 [ 0.99913311] [ 0.00197063]\n",
      "99 5.57137e-07 [ 0.99915397] [ 0.00192327]\n",
      "\n",
      "===Test===\n",
      "X: 5, Y: [ 4.99769306]\n",
      "X: 2.5, Y: [ 2.49980831]\n"
     ]
    }
   ],
   "source": [
    "# X와 Y의 상관관계를 분석하는 기초적인 선형회귀모델 만들기\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# name : 나중에 텐서보드등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙임\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "# 상관관계 분석을 위산 가설수실을 작성합니다.\n",
    "# y = W*x+b\n",
    "# W와 X가 행렬이 아님 ==> tf.matmul이 아닌 기본곱셈 사용\n",
    "hypothesis = W * X + b\n",
    "\n",
    "# 손실함수를 작성\n",
    "# mean(h-y)^2 : 예측값과 실제값의 거리를 비용함수로 정한다\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "# 텐서 플로우에 기본적으로 포함되어있는 함수를 이용, 경사하강법 최적화를 수행한다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# 비용을 최소화 하는것이 목표\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "# 세션을 생성하고 초기화\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 최적화를 100번 수행한다.\n",
    "    for step in range(100):\n",
    "        # sess.run을 통해 train_op와 cost그래프를 계산한다.\n",
    "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict을 통해 전달한다.\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y:y_data})\n",
    "        \n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "        \n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인\n",
    "    print(\"\\n===Test===\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X:5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
